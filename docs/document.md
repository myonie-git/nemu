#  RISC-V "MC" Memristor Crossbar Extension

## 1. Introduction

本项目旨在基于RISC-V构建一套可以高效支持忆阻器交叉阵列计算的指令集架构，简称RV64ME。RV64ME扩展是基于RV64I，RV64M两个基本RISCV指令集架构并结合了RV64V指令集架构的部分设计思想来进行设计的。RV64ME提供了利用忆阻器交叉阵列进行并行存内计算的指令集架构。RV64ME同时提供了对Resnet、MobileNet等传统ANN神经网络以及Spiking Resnet等新型SNN神经网络的推理指令的支持。


> TODO :

## 2. RV64ME programmers' model

### 2.1 概述

在本指令集架构中，我们使用忆阻器交叉阵列用于进行并行的数据处理，并希望利用忆阻器交叉阵列来代替传统的向量体积结构。但由于忆阻器交叉阵列中仍然需要利用向量寄存器来进行许多操作（如：bit lines 和 word lines的选通、数据的输入/读取等等），因此，我们保留了一部分向量体系结构的内容，保留的具体内容将在本章的剩余部分中进行介绍。

由于目前对于忆阻器的工艺水平有限，同时如果忆阻器交叉阵列过大，那么忆阻器的漏电流在提升的同时，还会造成忆阻器交叉阵列的效率过低。因此，我们仍然将忆阻器作为一种特殊的计算单元来进行使用。在进行计算时，我们仍然采用读取-载入-计算的方式来进行计算。

>  TODO: 忘记了读取-载入-计算这种计算结构叫什么了，回去查一下...


### 2.2 忆阻器交叉阵列以及向量寄存器长度设置


- 忆阻器交叉阵列大小设置：在本指令集中，我们设定忆阻器交叉阵列的大小为XLEN\*XLEN，XLEN参数可以在config.json文件中进行设置(为了设计方便，同时考虑到Resnet神经网络的最大卷积核为7*7，以及加法的便捷性，我们暂定XLEN=64，在之后的设计中，会进一步进行修改)

- 向量寄存器长度：由于在本指令集架构中，向量主要用于对忆阻器进行载入、忆阻器的行列选通、忆阻器计算结构的读取。因此，我们设置向量寄存器的长度与忆阻器相同，即XLEN*64位，可以存储XLEN个数字。

> TODO：是否需要设置长度为XLEN*1的寄存器用于wordlines的选通？

### 2.3 谓词寄存器

谓词寄存器沿用RV64V中关于谓词寄存器中的定义，其长度为XLEN位，谓词寄存器在本架构中的主要用途是[忆阻器加法 madd](#331-忆阻器加法指令-madd)的快速导入。具体的使用样例可以参照[残差连接 Residual](#44-残差链接residual)。


> TODO：去看看量化优化方法是怎么写的

### 2.4 小数计算

本架构计划实现定点计算而非浮点计算，为了保证神经网络的计算在可计算范围内，因此，我们对神经网络采用量化处理。

> TODO：目前先采用整数进行计算

### 2.5 忆阻器位置寄存器

忆阻器位置寄存器用于记录一组需要计算的数据在忆阻器中的位置，忆阻器寄存器长度为64位，每16位分别记录数据在忆阻器中的起始点的x坐标，y坐标以及长，宽。同时，我们设置了一系列[忆阻器位置寄存器设置指令](#25-忆阻器位置寄存器)以对其进行操作。

忆阻器位置寄存器共有四个，分别命名为m0,m1,m2,m3。

> TODO: 忆阻器寄存器是否可以与通用寄存器共用？但还需要考虑到lif参数寄存器的设置，也可以与lif参数寄存器共用一套编码。

## 3. RV64ME Instruction Set

### 3.1 概述

> TODO：待完成

### 3.2 忆阻器载入指令

#### 3.2.1 忆阻器载入指令mld

- 指令格式：mld ms1, vs1, v

### 3.3 忆阻器计算指令

#### 3.3.1 忆阻器加法指令 madd

- 指令格式：madd vd, ms1, ms2

#### 3.3.2 忆阻器减法指令 msub

- 指令格式：msub vd, ms1, ms2

#### 3.3.3 忆阻器按元素乘法指令 ewmul

- 指令格式：ewmul vd, ms1, vs1

#### 3.3.4 忆阻器点积指令 mdot

- 指令格式：mdot, vd, ms1, vs1

### 3.4 SNN相关指令

> TODO: 先实现ANN指令

#### 3.4.1 神经元指令 lif

### 3.5 其他指令

#### 3.5.1 向量寄存器合并指令 vgtm 

#### 3.5.2 单位向量寄存器生成指令 luv

#### 3.5.3 忆阻器位置寄存器设置指令 seta

### 3.6 RV64V 保留指令

## 4. Implementation of Neural Network Operator

> TODO

### 4.1 卷积Conv

### 4.2 最大池化层Max-Pooling

### 4.3 平均池化层Avg-Pooling

### 4.4 残差链接Residual

### 4.5 全连接层fc

### 4.6 批量归一化bn
